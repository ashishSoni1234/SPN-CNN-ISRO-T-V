{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12234684,"sourceType":"datasetVersion","datasetId":7708758},{"sourceId":12234698,"sourceType":"datasetVersion","datasetId":7708767},{"sourceId":12234703,"sourceType":"datasetVersion","datasetId":7708771},{"sourceId":12305444,"sourceType":"datasetVersion","datasetId":7756364},{"sourceId":12450712,"sourceType":"datasetVersion","datasetId":7854065}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Dataset Abstraction|\nimport json\nwith open('/kaggle/input/speedspn/speed/train.json') as f:\n    train_metadata = json.load(f)\n    print(train_metadata)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn,optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\nimport torchvision.transforms.functional as T\nfrom torchinfo import summary\nfrom scipy.io import loadmat\nfrom scipy.spatial.transform import Rotation as R\nfrom torch.amp import autocast\nfrom torch.cuda.amp import GradScaler\nimport numpy as np\nimport tqdm\nfrom torchvision import transforms, models, ops\nfrom torchvision.models.detection.anchor_utils import AnchorGenerator\nfrom torchvision.models.detection.rpn import RegionProposalNetwork, RPNHead\nfrom torchvision.models.detection.roi_heads import RoIHeads\nfrom torchvision.models.detection.faster_rcnn import TwoMLPHead, FastRCNNPredictor\nfrom torchvision.models.detection.transform import GeneralizedRCNNTransform","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_image_path_train = '/kaggle/input/speedspn/speed/images/trainval'\nbase_image_path_val   = '/kaggle/input/speedspn/speed/images/trainval'\n\ntrain_json_path = '/kaggle/input/speedspn/speed/train.json'\nval_json_path   = '/kaggle/input/speedspn/speed/val.json'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CustomTransform:\n    def __init__(self):\n        self.transform_preprocess = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n        ])\n        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        \n    def __call__(self, image, bbox=None, keypts=None):\n        orig_image = transforms.ToTensor()(image)\n        preprocess_image = self.transform_preprocess(image)\n        \n        return preprocess_image, orig_image, keypts\n\ntransform = CustomTransform()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.transforms as T\n\n# === Define image transformations ===\ntransform = T.Compose([\n    T.Resize((224, 224)),         # or another size depending on your model\n    T.ToTensor(),\n    T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # update if your model expects different stats\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom scipy.io import loadmat\nimport json\nfrom scipy.spatial.transform import Rotation as R\nimport torch.nn.functional as F\nimport torch\n\ndef get_discrete_classes(m):\n    \"\"\"\n    Computes m uniformly distributed random rotations parametrized as unit quaternions.\n    Args:\n        m (int): Number of random rotations to generate\n    Returns:\n        numpy.ndarray: Matrix of shape (m, 4) containing unit quaternions\n    \"\"\"\n    x0 = np.random.rand(m)\n    x1 = np.random.rand(m)\n    x2 = np.random.rand(m)\n\n    theta1 = 2 * np.pi * x1\n    theta2 = 2 * np.pi * x2\n\n    s1 = np.sin(theta1)\n    s2 = np.sin(theta2)\n    c1 = np.cos(theta1)\n    c2 = np.cos(theta2)\n\n    r1 = np.sqrt(1 - x0)\n    r2 = np.sqrt(x0)\n\n    quats = np.column_stack([s1 * r1, c1 * r1, s2 * r2, c2 * r2])\n    return quats\n\n\ndef _get_quat_bins(qPose, qClass, numNeighbors):\n    q = R.from_quat(qPose[[1, 2, 3, 0]])\n    qClass = R.from_quat(qClass[:, [1, 2, 3, 0]])\n\n    qDiff = q.inv() * qClass\n    qDiff = qDiff.as_quat()\n\n    angleVec = 2 * np.arccos(np.abs(qDiff[:, -1]))\n\n    sortIdx = np.argsort(angleVec)\n    nClasses = sortIdx[:numNeighbors]\n    nAngles = angleVec[nClasses]\n\n    nWeights = 1.0 - nAngles / np.pi**2\n    nWeights = nWeights / np.sum(nWeights)\n\n    return nClasses, nWeights\n\n\ndef softmax_cross_entropy_with_logits(logits, target, reduction='mean'):\n    loss = -torch.sum(target.detach() * F.log_softmax(logits, dim=1), dim=1)\n    if reduction == 'mean':\n        return loss.mean()\n    elif reduction == 'sum':\n        return loss.sum()\n    else:\n        return loss\n\n\ndef load_tango_3d_keypoints(mat_dir):\n    vertices = loadmat(mat_dir)['tango3Dpoints']\n    corners3D = np.transpose(np.array(vertices, dtype=np.float32))\n    return corners3D\n\n\ndef load_camera_intrinsics(camera_json):\n    with open(camera_json) as f:\n        cam = json.load(f)\n    cameraMatrix = np.array(cam['cameraMatrix'], dtype=np.float32)\n    distCoeffs = np.array(cam['distCoeffs'], dtype=np.float32)\n    return cameraMatrix, distCoeffs\n\n\ndef quat2dcm(q):\n    q = q / np.linalg.norm(q)\n    q0, q1, q2, q3 = q\n\n    dcm = np.zeros((3, 3))\n    dcm[0, 0] = 2 * q0 ** 2 - 1 + 2 * q1 ** 2\n    dcm[1, 1] = 2 * q0 ** 2 - 1 + 2 * q2 ** 2\n    dcm[2, 2] = 2 * q0 ** 2 - 1 + 2 * q3 ** 2\n\n    dcm[0, 1] = 2 * q1 * q2 + 2 * q0 * q3\n    dcm[0, 2] = 2 * q1 * q3 - 2 * q0 * q2\n    dcm[1, 0] = 2 * q1 * q2 - 2 * q0 * q3\n    dcm[1, 2] = 2 * q2 * q3 + 2 * q0 * q1\n    dcm[2, 0] = 2 * q1 * q3 + 2 * q0 * q2\n    dcm[2, 1] = 2 * q2 * q3 - 2 * q0 * q1\n\n    return dcm\n\n\ndef project_keypoints(q_vbs2tango, r_Vo2To_vbs, cameraMatrix, distCoeffs, keypoints):\n    if keypoints.shape[0] != 3:\n        keypoints = np.transpose(keypoints)\n\n    keypoints = np.vstack((keypoints, np.ones((1, keypoints.shape[1]))))\n    pose_mat = np.hstack((np.transpose(quat2dcm(q_vbs2tango)), np.expand_dims(r_Vo2To_vbs, 1)))\n    xyz = np.dot(pose_mat, keypoints)\n\n    x0, y0 = xyz[0, :] / xyz[2, :], xyz[1, :] / xyz[2, :]\n\n    r2 = x0 * x0 + y0 * y0\n    cdist = 1 + distCoeffs[0] * r2 + distCoeffs[1] * r2 * r2 + distCoeffs[4] * r2 * r2 * r2\n    x = x0 * cdist + distCoeffs[2] * 2 * x0 * y0 + distCoeffs[3] * (r2 + 2 * x0 * x0)\n    y = y0 * cdist + distCoeffs[2] * (r2 + 2 * y0 * y0) + distCoeffs[3] * 2 * x0 * y0\n\n    points2D = np.vstack((\n        cameraMatrix[0, 0] * x + cameraMatrix[0, 2],\n        cameraMatrix[1, 1] * y + cameraMatrix[1, 2]\n    ))\n\n    return points2D\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom scipy.io import loadmat\nimport tqdm\n\n# === Assume these functions are defined ===\n# _get_quat_bins, project_keypoints, load_tango_3d_keypoints, load_camera_intrinsics\n\nclass Speed(Dataset):\n    def __init__(self, images_dir, json_dir, transform=None):\n        self.images_dir = images_dir\n        self.json_dir = json_dir\n        self.transform = transform\n\n        self.num_classes = 5000\n        self.num_neighbors = 25\n\n        self.imagesList = []\n        self.yClassesList = []\n        self.yWeightsList = []\n        self.bboxList = []\n        self.keyptsList = []\n\n        # === Load auxiliary data ===\n        attClassesMAT = loadmat('/kaggle/input/attitue/attitudeClasses (1).mat')['qClass']\n        keypts3d = load_tango_3d_keypoints('/kaggle/input/tangojson/tangoPoints.mat')\n        cameraMatrix, distCoeffs = load_camera_intrinsics('/kaggle/input/camerajson/camera.json')\n\n        # === Load JSON annotations ===\n        with open(self.json_dir, 'r') as f:\n            annotations = json.load(f)\n\n        # Prepare lookup with corrected filenames\n        lookup = {\n            item['filename'].lower(): item  # No replace() needed now!\n            for item in annotations\n        }\n\n        # === Process each image file ===\n        for root, _, files in os.walk(self.images_dir):\n            for filename in files:\n                fname = filename.lower()\n                ann = lookup.get(fname)\n                if ann is None:\n                    continue\n\n                full_path = os.path.join(root, filename)\n                self.imagesList.append(full_path)\n\n                q_vbs2tango = np.array(ann[\"q_vbs2tango\"], dtype=np.float32)\n                r_Vo2To_vbs = np.array(ann[\"r_Vo2To_vbs_true\"], dtype=np.float32)\n\n                attClasses, attWeights = _get_quat_bins(q_vbs2tango, attClassesMAT, self.num_neighbors)\n\n                yClasses = np.zeros(self.num_classes, dtype=np.float32)\n                yClasses[attClasses] = 1. / self.num_neighbors\n\n                yWeights = np.zeros(self.num_classes, dtype=np.float32)\n                yWeights[attClasses] = attWeights\n\n                self.yClassesList.append(torch.from_numpy(yClasses))\n                self.yWeightsList.append(torch.from_numpy(yWeights))\n\n                # Project keypoints and compute bounding box\n                keypts2d = project_keypoints(q_vbs2tango, r_Vo2To_vbs, cameraMatrix, distCoeffs, keypts3d)\n\n                xmin = np.min(keypts2d[0])\n                xmax = np.max(keypts2d[0])\n                ymin = np.min(keypts2d[1])\n                ymax = np.max(keypts2d[1])\n\n                # Add margin\n                margin_x = 0.05 * (xmax - xmin)\n                margin_y = 0.05 * (ymax - ymin)\n\n                bbox = [\n                    max(0, xmin - margin_x),\n                    xmax + margin_x,\n                    max(0, ymin - margin_y),\n                    ymax + margin_y\n                ]\n\n                self.bboxList.append(torch.tensor(bbox, dtype=torch.float32))\n                self.keyptsList.append(torch.tensor(keypts2d, dtype=torch.float32))\n\n    def __getitem__(self, idx):\n        image_path = self.imagesList[idx]\n        image = Image.open(image_path).convert(\"RGB\")\n\n        if self.transform:\n            image = self.transform(image)\n\n        return {\n            'image': image,\n            'filename': os.path.basename(image_path),\n            'y_class': self.yClassesList[idx],\n            'y_weight': self.yWeightsList[idx],\n            'bbox': self.bboxList[idx],\n            'keypoints': self.keyptsList[idx]\n        }\n\n    def __len__(self):\n        return len(self.imagesList)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = Speed(images_dir=base_image_path_train, json_dir=train_json_path, transform=transform)\nprint(\"✅ Length of train_dataset:\", len(train_dataset))\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# === Create dataset instances ===\ntrain_dataset = Speed(images_dir=base_image_path_train, json_dir=train_json_path, transform=transform)\nval_dataset   = Speed(images_dir=base_image_path_val,   json_dir=val_json_path,   transform=transform)\n\n# === Create DataLoaders ===\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=32,         # change as needed\n    shuffle=True,\n    num_workers=2,         # set to 0 for Kaggle or increase for speed in local\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    dataset=val_dataset,\n    batch_size=32,\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DetectionWrapperDataset(Dataset):\n    def __init__(self, base_dataset):\n        self.base_dataset = base_dataset\n\n    def __len__(self):\n        return len(self.base_dataset)\n\n    def __getitem__(self, idx):\n        sample = self.base_dataset[idx]\n        image = sample['image']\n        bbox = sample['bbox']\n\n        box = torch.tensor([[bbox[0], bbox[2], bbox[1], bbox[3]]], dtype=torch.float32)  # [x_min, y_min, x_max, y_max]\n        label = torch.tensor([1], dtype=torch.int64)  # class 1 = object present\n\n        target = {'boxes': box, 'labels': label}\n\n        return image, target","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detection_collate_fn(batch):\n    images = [item[0] for item in batch]\n    targets = [item[1] for item in batch]\n    return images, targets\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\nfrom torchvision import models, ops\nfrom torchvision.models.detection.rpn import RegionProposalNetwork, RPNHead\nfrom torchvision.models.detection.roi_heads import RoIHeads\nfrom torchvision.models.detection.anchor_utils import AnchorGenerator\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor, TwoMLPHead\nfrom torchvision.models.detection.transform import GeneralizedRCNNTransform\n\nclass MobileNetRPNRoI(nn.Module):\n    def __init__(self, num_classes=2, freeze_backbone=True):\n        super().__init__()\n        \n        # === Load MobileNetV2 ===\n        mobilenet = models.mobilenet_v2(weights=None)\n        self.backbone = mobilenet.features\n        self.out_channels = 1280\n        self.backbone.out_channels = self.out_channels\n\n        # === Freeze backbone if requested ===\n        if freeze_backbone:\n            for param in self.backbone.parameters():\n                param.requires_grad = False\n\n        # === Image transformer ===\n        self.transform = GeneralizedRCNNTransform(\n            min_size=224,\n            max_size=224,\n            image_mean=[0.485, 0.456, 0.406],\n            image_std=[0.229, 0.224, 0.225]\n        )\n\n        # === RPN: Region Proposal Network ===\n        anchor_generator = AnchorGenerator(\n            sizes=((32, 64, 128),),\n            aspect_ratios=((0.5, 1.0, 2.0),)\n        )\n\n        rpn_head = RPNHead(\n            in_channels=self.out_channels,\n            num_anchors=anchor_generator.num_anchors_per_location()[0]\n        )\n\n        self.rpn = RegionProposalNetwork(\n            anchor_generator=anchor_generator,\n            head=rpn_head,\n            fg_iou_thresh=0.7,\n            bg_iou_thresh=0.3,\n            batch_size_per_image=256,\n            positive_fraction=0.5,\n            pre_nms_top_n={'training': 2000, 'testing': 1000},\n            post_nms_top_n={'training': 2000, 'testing': 300},\n            nms_thresh=0.7\n        )\n\n        # === RoI Heads ===\n        roi_pool = ops.MultiScaleRoIAlign(\n            featmap_names=['0'],\n            output_size=7,\n            sampling_ratio=2\n        )\n\n        box_head = TwoMLPHead(\n            in_channels=self.out_channels * 7 * 7,\n            representation_size=1024\n        )\n\n        box_predictor = FastRCNNPredictor(in_channels=1024, num_classes=num_classes)\n\n        self.roi_heads = RoIHeads(\n            box_roi_pool=roi_pool,\n            box_head=box_head,\n            box_predictor=box_predictor,\n            fg_iou_thresh=0.5,\n            bg_iou_thresh=0.5,\n            batch_size_per_image=128,\n            positive_fraction=0.25,\n            bbox_reg_weights=None,\n            score_thresh=0.05,\n            nms_thresh=0.5,\n            detections_per_img=100\n        )\n\n    def forward(self, images, targets=None):\n        # Apply transform\n        images, targets = self.transform(images, targets)\n\n        # Extract features\n        features = self.backbone(images.tensors)  # shape: [B, 1280, H, W]\n        features = {\"0\": features}\n\n        # RPN proposals\n        proposals, rpn_losses = self.rpn(images, features, targets)\n\n        # RoI heads (bbox refinement/classification)\n        detections, roi_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n\n        # Aggregate losses\n        losses = {}\n        losses.update(rpn_losses)\n        losses.update(roi_losses)\n\n        if self.training:\n            return losses\n        return detections\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Instantiate the model with frozen backbone\nmodel = MobileNetRPNRoI(num_classes=2, freeze_backbone=True).to(device)\n\n# Optimizer only updates parameters that require gradients\noptimizer = torch.optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=1e-4\n)\n\n# Wrap datasets\ntrain_wrapped = DetectionWrapperDataset(train_dataset)\nval_wrapped = DetectionWrapperDataset(val_dataset)\n\n# DataLoaders\ntrain_loader = DataLoader(train_wrapped, batch_size=4, shuffle=True, collate_fn=detection_collate_fn)\nval_loader = DataLoader(val_wrapped, batch_size=4, shuffle=False, collate_fn=detection_collate_fn)\n\n# Training loop\nfor epoch in range(75):\n    model.train()\n    epoch_loss = 0.0\n\n    for images, targets in tqdm.tqdm(train_loader):\n        images = [img.to(device) for img in images]\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        epoch_loss += losses.item()\n\n    print(f\"[Epoch {epoch + 1}] Loss: {epoch_loss:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"mobilenet_rpn_roi.pth\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_position_spn(q_vbs2tango, bbox, corners3D, cameraMatrix, distCoeffs=np.zeros((1,5))):\n    ''' Compute position vector for SPN model\n    Arguments:\n        q_vbs2tango: (4,) numpy.ndarray - predicted unit quaternion (scalar-first)\n        bbox:        (4,) numpy.ndarray - bounding box [xmin, xmax, ymin, ymax] (pix)\n        ...\n    Returns:\n        r_Vo2To_vbs: (3,) numpy.ndarray - predicted position vector (m)\n    '''\n    maxModelLength = 1.246 # [m] for Tango\n\n    # Bounding box decomposition\n    xmin, ymin, width, height = bbox[0], bbox[2], bbox[1]-bbox[0], bbox[3]-bbox[2]\n\n    # Initial position guess based on similar triangles\n    boxSize   = np.sqrt(width**2 + height**2)\n    boxCenter = np.array([xmin + width/2.0, ymin + height/2.0])\n    offsetPx  = np.array([boxCenter[0] - cameraMatrix[0,2],\n                          boxCenter[1] - cameraMatrix[1,2]])\n    az = np.arctan(offsetPx[0]/cameraMatrix[0,0]) # [rad]\n    el = np.arctan(offsetPx[1]/cameraMatrix[1,1])\n    range_wge = cameraMatrix[0,0] * maxModelLength / boxSize # [m]\n    Ry = R.from_euler('y', -az).as_matrix()\n    Rx = R.from_euler('x', -el).as_matrix()\n    r_Vo2To_vbs = Ry @ Rx @ np.reshape(np.array([0, 0, range_wge]), (3,1))\n\n    # NEWTON's METHOD\n    maxIter = 50\n    tolerance = 5e-10\n    iter = 0\n    dx = 1 + 1e-15\n\n    # Initialize betas\n    beta_old = np.squeeze(r_Vo2To_vbs)\n\n    while dx > tolerance and iter <= maxIter:\n        # Compute extreme reprojected points in VBS frame\n        r_Vo2X_vbs = _compute_extremal_points(q_vbs2tango, beta_old, corners3D, cameraMatrix) # [4 x 3]\n\n        # Compute update to beta\n        r = _calc_residuals(r_Vo2X_vbs, cameraMatrix, distCoeffs, beta_old, bbox)\n        J = _calc_jacobian(r_Vo2X_vbs, cameraMatrix, distCoeffs, beta_old)\n        beta_new = beta_old - np.squeeze(np.linalg.inv(np.transpose(J) @ J) @ np.transpose(J) @ np.reshape(r, (4,1)))\n\n        # Compute change between new and oldbeta\n        dx = np.linalg.norm(beta_new - beta_old)\n\n        # Updates\n        iter = iter + 1\n        beta_old = beta_new\n\n    r_Vo2To_vbs = beta_new\n\n    return r_Vo2To_vbs\n\ndef _compute_extremal_points(q_vbs2tango, r_Vo2To_vbs, tangoPoints, cameraMatrix):\n    ''' Compute the extremal points of the Tango model given orientation and position estimates '''\n    reprImagePoints = project_keypoints(q_vbs2tango, r_Vo2To_vbs,\n                                cameraMatrix, np.zeros((5,)), tangoPoints)\n    idx1 = np.argmin(reprImagePoints[0]) # xmin\n    idx2 = np.argmin(reprImagePoints[1]) # ymin\n    idx3 = np.argmax(reprImagePoints[0]) # xmax\n    idx4 = np.argmax(reprImagePoints[1]) # ymax\n\n    if tangoPoints.shape[0] != 3:\n        tangoPoints = np.transpose(tangoPoints)\n    tangoPoints_vbs = np.transpose(quat2dcm(q_vbs2tango)) @ tangoPoints\n\n    r_Vo2X_vbs = np.zeros((4, 3))\n    r_Vo2X_vbs[0] = tangoPoints_vbs[:,idx1] # left-most point\n    r_Vo2X_vbs[1] = tangoPoints_vbs[:,idx3] # right-most point\n    r_Vo2X_vbs[2] = tangoPoints_vbs[:,idx2] # top-most point\n    r_Vo2X_vbs[3] = tangoPoints_vbs[:,idx4] # bottom-most point\n\n    return r_Vo2X_vbs\n\ndef _calc_residuals(r_Vo2X_vbs, cameraMatrix, distCoeffs, r_Vo2To_vbs, bbox):\n    ''' Compute residuals of projected extremal points against the bounding box '''\n    Tx, Ty, Tz = r_Vo2To_vbs\n    Bx1, Bx2, By1, By2 = bbox\n\n    xs, ys = [], []\n    for ii in range(4):\n        # Project\n        Rx, Ry, Rz = r_Vo2X_vbs[ii]\n        x0 = (Rx + Tx) / (Rz + Tz)\n        y0 = (Ry + Ty) / (Rz + Tz)\n\n        # Distortion\n        r2 = x0*x0 + y0*y0\n        cdist = 1 + distCoeffs[0]*r2 + distCoeffs[1]*r2*r2 + distCoeffs[4]*r2*r2*r2\n        x  = x0*cdist + distCoeffs[2]*2*x0*y0 + distCoeffs[3]*(r2 + 2*x0*x0)\n        y  = y0*cdist + distCoeffs[2]*(r2 + 2*y0*y0) + distCoeffs[3]*2*x0*y0\n\n        # Apply camera\n        xs.append(cameraMatrix[0,0]*x + cameraMatrix[0,2])\n        ys.append(cameraMatrix[1,1]*y + cameraMatrix[1,2])\n\n    # Residuals\n    r1 = xs[0] - Bx1\n    r2 = xs[1] - Bx2\n    r3 = ys[2] - By1\n    r4 = ys[3] - By2\n\n    return np.array([r1, r2, r3, r4])\n\ndef _calc_jacobian(r_Vo2X_vbs, cameraMatrix, distCoeffs, r_Vo2To_vbs):\n    ''' Compute jacobian of the residuals.\n        Camera distortion coefficients are neglected at the moment.\n    '''\n    fx, fy = cameraMatrix[0,0], cameraMatrix[1,1]\n    Tx, Ty, Tz = r_Vo2To_vbs\n    Rx_left, Rz_left = r_Vo2X_vbs[0,0], r_Vo2X_vbs[0,2]\n    Rx_right, Rz_right = r_Vo2X_vbs[1,0], r_Vo2X_vbs[1,2]\n    Ry_top, Rz_top = r_Vo2X_vbs[2,1], r_Vo2X_vbs[2,2]\n    Ry_bot, Rz_bot = r_Vo2X_vbs[3,1], r_Vo2X_vbs[3,2]\n\n    # Left-most image feature\n    dr1db1 = fx / (Rz_left + Tz)\n    dr1db2 = 0\n    dr1db3 = -fx * (Rx_left + Tx) / (Rz_left + Tz)**2\n\n    # Right-most iamge feature\n    dr2db1 = fx / (Rz_right + Tz)\n    dr2db2 = 0\n    dr2db3 = -fx * (Rx_right + Tx) / (Rz_right + Tz)**2\n\n    # Top-most image feature\n    dr3db1 = 0\n    dr3db2 = fy / (Rz_top + Tz)\n    dr3db3 = -fy * (Ry_top + Ty) / (Rz_top + Tz)**2\n\n    # Bottom-most image feature\n    dr4db1 = 0\n    dr4db2 = fy / (Rz_bot + Tz)\n    dr4db3 = -fy * (Ry_bot + Ty) / (Rz_bot + Tz)**2\n\n    # Jacobian\n    J = np.array([[dr1db1, dr1db2, dr1db3],\n                  [dr2db1, dr2db2, dr2db3],\n                  [dr3db1, dr3db2, dr3db3],\n                  [dr4db1, dr4db2, dr4db3]], dtype=np.float32)\n\n    return J","metadata":{"execution":{"iopub.execute_input":"2025-07-10T23:00:58.557481Z","iopub.status.busy":"2025-07-10T23:00:58.557131Z","iopub.status.idle":"2025-07-10T23:00:58.578237Z","shell.execute_reply":"2025-07-10T23:00:58.577583Z","shell.execute_reply.started":"2025-07-10T23:00:58.557451Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def valid_spn(epoch, model, data_loader, cameraMatrix, distCoeffs, corners3D, writer, device, qClass):\n    ''' Minimal SPN Validation: only Rotation & Translation error '''\n\n    model.eval()\n    total_q_error = 0.0\n    total_t_error = 0.0\n    total_samples = 0\n\n    for idx, (images, bbox, q_gt, t_gt) in enumerate(data_loader):\n        B = images.shape[0]\n        total_samples += B\n\n        with torch.no_grad():\n            _, weights = model(images.to(device))\n            topWeights, topClasses = torch.topk(weights, 5, dim=1)  # hardcoded top-5\n            topWeights = torch.softmax(topWeights, dim=1)\n\n        for b in range(B):\n            qs_pr = qClass[topClasses[b].cpu()]        # [5, 4]\n            q_weights = topWeights[b].cpu()            # [5]\n            q_pr = weighted_mean_quaternion(qs_pr, q_weights)\n            q_pr /= np.linalg.norm(q_pr)\n\n            t_pr = compute_position_spn(q_pr, bbox[b].numpy(), corners3D, cameraMatrix, distCoeffs)\n\n            q_gt_i = q_gt[b].numpy()\n            t_gt_i = t_gt[b].numpy()\n\n            err_q = error_orientation(q_pr, q_gt_i)\n            err_t = error_translation(t_pr, t_gt_i)\n\n            total_q_error += err_q\n            total_t_error += err_t\n\n    # Final average\n    avg_q_error = total_q_error / total_samples\n    avg_t_error = total_t_error / total_samples\n\n    print(f\"\\n Validation Epoch {epoch}:\")\n    print(f\" Rotation Error (deg)   : {avg_q_error:.2f}\")\n    print(f\" Translation Error (m)  : {avg_t_error:.4f}\")\n\n    return {\n        'eR': avg_q_error,\n        'eT': avg_t_error\n    }","metadata":{"execution":{"iopub.execute_input":"2025-07-10T23:00:58.579277Z","iopub.status.busy":"2025-07-10T23:00:58.578959Z","iopub.status.idle":"2025-07-10T23:00:58.593331Z","shell.execute_reply":"2025-07-10T23:00:58.592757Z","shell.execute_reply.started":"2025-07-10T23:00:58.579254Z"},"trusted":true},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def weighted_mean_quaternion(qs, weights=None):\n    ''' Compute weighted mean of N unit quaternions.\n    Arguments:\n        qs: (N, 4) or (4, N) numpy.ndarray - unit quaternions (scalar-first)\n    Returns:\n        q: (4,) numpy.ndarray - weighted mean unit quaternion (scalar-first)\n    '''\n    # Size check\n    if qs.shape[1] != 4:\n        qs = np.transpose(qs) # (N,4)\n\n    # Scipy uses scalar-last convention\n    qs = qs[:,[1,2,3,0]]\n\n    # Weights?\n    if weights is None:\n        weights = np.ones((qs.shape[0],), dtype=np.float32)\n\n    # Quaternions to rotation matrices\n    Rs = R.from_quat(qs)\n\n    # Weighted average\n    q = Rs.mean(weights).as_quat() # (4,)\n\n    # Back to scalar-first convention\n    q = q[[3,0,1,2]]\n\n    return q","metadata":{"execution":{"iopub.execute_input":"2025-07-10T23:00:58.594246Z","iopub.status.busy":"2025-07-10T23:00:58.594074Z","iopub.status.idle":"2025-07-10T23:00:58.608925Z","shell.execute_reply":"2025-07-10T23:00:58.608256Z","shell.execute_reply.started":"2025-07-10T23:00:58.594233Z"},"trusted":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def error_orientation(q_pr, q_gt):\n    # q must be [qvec, qcos]\n    q_pr = np.reshape(q_pr, (4,))\n    q_gt = np.reshape(q_gt, (4,))\n\n    qdot = np.abs(np.dot(q_pr, q_gt))\n    qdot = np.minimum(qdot, 1.0)\n    return np.rad2deg(2*np.arccos(qdot)) # [deg","metadata":{"execution":{"iopub.execute_input":"2025-07-10T23:00:58.609724Z","iopub.status.busy":"2025-07-10T23:00:58.609540Z","iopub.status.idle":"2025-07-10T23:00:58.624411Z","shell.execute_reply":"2025-07-10T23:00:58.623731Z","shell.execute_reply.started":"2025-07-10T23:00:58.609694Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"def error_translation(t_pr, t_gt):\n    t_pr = np.reshape(t_pr, (3,))\n    t_gt = np.reshape(t_gt, (3,))\n\n    return np.sqrt(np.sum(np.square(t_gt - t_pr)))","metadata":{"execution":{"iopub.execute_input":"2025-07-10T23:00:58.625446Z","iopub.status.busy":"2025-07-10T23:00:58.625266Z","iopub.status.idle":"2025-07-10T23:00:58.635161Z","shell.execute_reply":"2025-07-10T23:00:58.634457Z","shell.execute_reply.started":"2025-07-10T23:00:58.625432Z"},"trusted":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport tqdm\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom scipy.io import loadmat\n\ndef load_tango_3d_keypoints(mat_dir):\n    vertices = loadmat(mat_dir)['tango3Dpoints']\n    return np.transpose(vertices.astype(np.float32))\n\ndef load_camera_intrinsics(camera_json):\n    with open(camera_json) as f:\n        cam = json.load(f)\n    cameraMatrix = np.array(cam['cameraMatrix'], dtype=np.float32)\n    distCoeffs = np.array(cam['distCoeffs'], dtype=np.float32)\n    return cameraMatrix, distCoeffs\n\nclass Speed(Dataset):\n    def __init__(self, images_dir, json_dir, transform=None):\n        self.images_dir = images_dir\n        self.json_dir = json_dir\n        self.transform = transform\n\n        self.num_classes = 5000\n        self.num_neighbors = 5\n\n        self.imagesList = []\n        self.yClassesList = []\n        self.yWeightsList = []\n        self.bboxList = []\n        self.keyptsList = []\n        self.qList = []\n        self.tList = []\n\n        attClassesMAT = loadmat('/kaggle/input/atlitude/attitudeClasses.mat')['qClass']\n        keypts3d = load_tango_3d_keypoints('/kaggle/input/tangopoint/tangoPoints.mat')\n        cameraMatrix, distCoeffs = load_camera_intrinsics('/kaggle/input/speed/speedplusv2/camera.json')\n\n        with open(self.json_dir, 'r') as f:\n            annotations = json.load(f)\n        lookup = {item['filename']: item for item in annotations}\n\n        for filename in tqdm.tqdm(os.listdir(self.images_dir)):\n            if filename not in lookup:\n                continue\n\n            self.imagesList.append(os.path.join(self.images_dir, filename))\n            ann = lookup[filename]\n\n            q_vbs2tango = np.array(ann[\"q_vbs2tango\"], dtype=np.float32)\n            r_Vo2To_vbs = np.array(ann['r_Vo2To_vbs_true'], dtype=np.float32)\n\n            attClasses, attWeights = _get_quat_bins(q_vbs2tango, attClassesMAT, self.num_neighbors)\n\n            yClasses = np.zeros(self.num_classes, dtype=np.float32)\n            yClasses[attClasses] = 1. / self.num_neighbors\n            yWeights = np.zeros(self.num_classes, dtype=np.float32)\n            yWeights[attClasses] = attWeights\n\n            self.yClassesList.append(torch.from_numpy(yClasses))\n            self.yWeightsList.append(torch.from_numpy(yWeights))\n\n            keypts2d = np.random.rand(2, 11) * 100  # placeholder if project_keypoints not defined\n            xmin, xmax = np.min(keypts2d[0]), np.max(keypts2d[0])\n            ymin, ymax = np.min(keypts2d[1]), np.max(keypts2d[1])\n            width, height = xmax - xmin, ymax - ymin\n            bbox = [max(0, xmin - 0.05 * width), xmax + 0.05 * width,\n                    max(0, ymin - 0.05 * height), ymax + 0.05 * height]\n\n            self.bboxList.append(bbox)\n            self.keyptsList.append(torch.tensor(keypts2d, dtype=torch.float32))\n            self.qList.append(q_vbs2tango)\n            self.tList.append(r_Vo2To_vbs)\n\n    def __getitem__(self, idx):\n        image_path = self.imagesList[idx]\n        image = Image.open(image_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n\n        return {\n            'image': image,\n            'filename': os.path.basename(image_path),\n            'y_class': self.yClassesList[idx],\n            'y_weight': self.yWeightsList[idx],\n            'bbox': torch.tensor(self.bboxList[idx], dtype=torch.float32),\n            'keypoints': self.keyptsList[idx],\n            'quat_gt': torch.tensor(self.qList[idx], dtype=torch.float32),\n            'trans_gt': torch.tensor(self.tList[idx], dtype=torch.float32)\n        }\n\n    def __len__(self):\n        return len(self.imagesList)","metadata":{"execution":{"iopub.execute_input":"2025-07-10T23:00:58.636240Z","iopub.status.busy":"2025-07-10T23:00:58.635973Z","iopub.status.idle":"2025-07-10T23:00:58.652609Z","shell.execute_reply":"2025-07-10T23:00:58.651992Z","shell.execute_reply.started":"2025-07-10T23:00:58.636219Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom scipy.io import loadmat\n\n# === Load Data\nqClass = loadmat('/kaggle/input/atlitude/attitudeClasses.mat')['qClass']\ncorners3D = load_tango_3d_keypoints('/kaggle/input/tangopoint/tangoPoints.mat')\ncameraMatrix, distCoeffs = load_camera_intrinsics('/kaggle/input/speed/speedplusv2/camera.json')\n\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\nval_dataset = Speed(\n    images_dir='/kaggle/input/speedsplit/speed/images/trainval',\n    json_dir='/kaggle/input/speedsplit/speed/val.json',\n    transform=val_transform\n)\n\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n\n# === Validation Function\ndef valid_spn(epoch, model, data_loader, cameraMatrix, distCoeffs, corners3D, writer, device, qClass):\n    model.eval()\n    total_q_error, total_t_error, total_samples = 0, 0, 0\n\n    for batch in data_loader:\n        images = batch['image'].to(device)\n        bbox = batch['bbox']\n        q_gt = batch['quat_gt']\n        t_gt = batch['trans_gt']\n        B = images.shape[0]\n        total_samples += B\n\n        with torch.no_grad():\n            _, weights = model(images)\n            topWeights, topClasses = torch.topk(weights, 5, dim=1)\n            topWeights = torch.softmax(topWeights, dim=1)\n\n        for b in range(B):\n            qs_pr = qClass[topClasses[b].cpu()]      # [5, 4]\n            q_weights = topWeights[b].cpu()          # [5]\n            q_pr = weighted_mean_quaternion(qs_pr, q_weights)\n            q_pr = q_pr / np.linalg.norm(q_pr)\n            t_pr = compute_position_spn(q_pr, bbox[b].numpy(), corners3D, cameraMatrix, distCoeffs)\n            err_q = error_orientation(q_pr, q_gt[b].numpy())\n            err_t = error_translation(t_pr, t_gt[b].numpy())\n            total_q_error += err_q\n            total_t_error += err_t\n\n    avg_q = total_q_error / total_samples\n    avg_t = total_t_error / total_samples\n    print(f\"\\nEpoch {epoch} - Rotation Error: {avg_q:.2f} deg | Translation Error: {avg_t:.4f} m\")\n    return {'eR': avg_q, 'eT': avg_t}\n\n# === Model and Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\n# === Run\nepoch = 0\nperformances = valid_spn(\n    epoch=epoch,\n    model=model,\n    data_loader=val_loader,\n    cameraMatrix=cameraMatrix,\n    distCoeffs=distCoeffs,\n    corners3D=corners3D,\n    writer=None,\n    device=device,\n    qClass=qClass\n)\n\nprint(\"\\nFinal Validation Results:\")\nprint(f\"Rotation Error (deg):   {performances['eR']:.2f}\")\nprint(f\"Translation Error (m):  {performances['eT']:.4f}\")","metadata":{"execution":{"iopub.execute_input":"2025-07-10T23:00:58.654143Z","iopub.status.busy":"2025-07-10T23:00:58.653396Z","iopub.status.idle":"2025-07-10T23:02:09.228078Z","shell.execute_reply":"2025-07-10T23:02:09.227184Z","shell.execute_reply.started":"2025-07-10T23:00:58.654124Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 12000/12000 [00:18<00:00, 643.53it/s]\n"]},{"name":"stdout","output_type":"stream","text":["\n","Epoch 0 - Rotation Error: 39.74 deg | Translation Error: 26.2184 m\n","\n","Final Validation Results:\n","Rotation Error (deg):   39.74\n","Translation Error (m):  26.2184\n"]}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}